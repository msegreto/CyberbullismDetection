{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0189fb8c",
   "metadata": {},
   "source": [
    "# Multiclass Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415013d0",
   "metadata": {},
   "source": [
    "## Library and Dataset import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78edc31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import resample\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import pickle\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from utils.w2v_feature_extraction import compute_w2v_features\n",
    "\n",
    "df_train = pd.read_csv(\"../dataset/training_set.csv\")\n",
    "df_train.head()\n",
    "X_text = df_train[\"text\"]\n",
    "y_binary = df_train[\"multiclass_label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0abb5f1",
   "metadata": {},
   "source": [
    "## Not_cyberbulling pruning & Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40e11deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Classes: {'age': 0, 'ethnicity': 1, 'gender': 2, 'other_cyberbullying': 3, 'religion': 4}\n",
      "Classe Distribution:\n",
      "multiclass_label\n",
      "religion               6398\n",
      "age                    6393\n",
      "ethnicity              6368\n",
      "gender                 6354\n",
      "other_cyberbullying    6081\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# --- 1. not_cyberbullying pruning ---\n",
    "\n",
    "df_filtered = df_train[df_train[\"multiclass_label\"] != \"not_cyberbullying\"].copy()\n",
    "\n",
    "label2id = {label: idx for idx, label in enumerate(sorted(df_filtered[\"multiclass_label\"].unique()))}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "df_filtered[\"label_id\"] = df_filtered[\"multiclass_label\"].map(label2id)\n",
    "\n",
    "X_text_pruned = df_filtered[\"text\"]\n",
    "y_multiclass = df_filtered[\"label_id\"]\n",
    "\n",
    "print(\"Final Classes:\", label2id)\n",
    "print(\"Classe Distribution:\")\n",
    "print(df_filtered[\"multiclass_label\"].value_counts())\n",
    "\n",
    "# --- 2. Feature extraction ---\n",
    "\n",
    "# BoW\n",
    "bow_vectorizer = CountVectorizer(max_features=350)\n",
    "X_bow = bow_vectorizer.fit_transform(X_text_pruned)\n",
    "with open(\"../model/bow_vocabulary.pkl\", \"wb\") as f:\n",
    "    pickle.dump(bow_vectorizer.vocabulary_, f)\n",
    "\n",
    "# TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=350)\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(X_text_pruned)\n",
    "with open(\"../model/tfidf_vocabulary.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tfidf_vectorizer.vocabulary_, f)\n",
    "\n",
    "# Load Word2Vec models\n",
    "model1 = Word2Vec.load(\"../model/word2vec_model1.model\")\n",
    "model2 = Word2Vec.load(\"../model/word2vec_model2.model\")\n",
    "\n",
    "X_w2v1 = compute_w2v_features(X_text_pruned, model1, model1.vector_size)\n",
    "X_w2v2 = compute_w2v_features(X_text_pruned, model2, model2.vector_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48ab81c",
   "metadata": {},
   "source": [
    "## GRID search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5bbd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assicurati che il path esista\n",
    "os.makedirs(\"../model\", exist_ok=True)\n",
    "results_list = []\n",
    "\n",
    "# Parametri aggiornati per multiclass\n",
    "param_grid = {\n",
    "    \"LogisticRegression\": {\n",
    "        \"model__C\": [0.01, 0.1, 1, 10],\n",
    "        \"model__penalty\": [\"l2\"],\n",
    "        \"model__solver\": [\"lbfgs\"],\n",
    "        \"model__multi_class\": [\"multinomial\"]\n",
    "    },\n",
    "    \"SVM\": {\n",
    "        \"model__C\": [0.01, 0.1, 1, 10],\n",
    "        \"model__kernel\": [\"linear\", \"rbf\"],\n",
    "        \"model__gamma\": [\"scale\", \"auto\"]\n",
    "    },\n",
    "    \"RandomForest\": {\n",
    "        \"model__n_estimators\": [100, 200, 400, 500],\n",
    "        \"model__max_depth\": [None, 10, 20],\n",
    "        \"model__min_samples_split\": [2, 5],\n",
    "        \"model__min_samples_leaf\": [1, 2]\n",
    "    }\n",
    "}\n",
    "\n",
    "models = {\n",
    "    \"LogisticRegression\": LogisticRegression(max_iter=1000),\n",
    "    \"SVM\": SVC(),\n",
    "    \"RandomForest\": RandomForestClassifier()\n",
    "}\n",
    "\n",
    "datasets = {\n",
    "    \"BoW\": X_bow,\n",
    "    \"TF-IDF\": X_tfidf,\n",
    "    \"W2V-1\": X_w2v1,\n",
    "    \"W2V-2\": X_w2v2\n",
    "}\n",
    "\n",
    "# Etichette multiclass\n",
    "y = y_multiclass\n",
    "\n",
    "# Cross-validation setup\n",
    "cv_strategy = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Scoring multiclass (f1-weighted: utile se le classi non sono perfettamente bilanciate)\n",
    "scoring = {\n",
    "    \"accuracy\": \"accuracy\",\n",
    "    \"precision\": make_scorer(precision_score, average=\"weighted\", zero_division=0),\n",
    "    \"recall\": make_scorer(recall_score, average=\"weighted\", zero_division=0),\n",
    "    \"f1\": make_scorer(f1_score, average=\"weighted\", zero_division=0)\n",
    "}\n",
    "\n",
    "for vectorizer_name, X in datasets.items():\n",
    "    for model_name, model in models.items():\n",
    "        \n",
    "        steps = []\n",
    "        if \"W2V\" in vectorizer_name:\n",
    "            steps.append((\"scaler\", StandardScaler()))\n",
    "        steps.append((\"model\", model))\n",
    "        pipeline = Pipeline(steps)\n",
    "\n",
    "        grid = GridSearchCV(\n",
    "            estimator=pipeline,\n",
    "            param_grid=param_grid[model_name],\n",
    "            cv=cv_strategy,\n",
    "            scoring=scoring,\n",
    "            refit=\"f1\",\n",
    "            n_jobs=-1,\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        grid.fit(X, y)\n",
    "\n",
    "        model_path = f\"../model/grid_search_multiclass/{model_name}_{vectorizer_name}_multiclass.pkl\"\n",
    "        joblib.dump(grid.best_estimator_, model_path)\n",
    "\n",
    "        best_idx = grid.best_index_\n",
    "        results_list.append({\n",
    "            \"model\": model_name,\n",
    "            \"vectorizer\": vectorizer_name,\n",
    "            \"accuracy\": grid.cv_results_[\"mean_test_accuracy\"][best_idx],\n",
    "            \"precision\": grid.cv_results_[\"mean_test_precision\"][best_idx],\n",
    "            \"recall\": grid.cv_results_[\"mean_test_recall\"][best_idx],\n",
    "            \"f1\": grid.cv_results_[\"mean_test_f1\"][best_idx]\n",
    "        })\n",
    "\n",
    "with open(\"../model/grid_search_multiclass/results_multiclass.json\", \"w\") as f:\n",
    "    json.dump(results_list, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd4c40a",
   "metadata": {},
   "source": [
    "## K-fold Cross Validation and Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff1714e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "datasets = {\n",
    "    \"BoW\": X_bow,\n",
    "    \"TF-IDF\": X_tfidf,\n",
    "    \"W2V-1\": X_w2v1,\n",
    "    \"W2V-2\": X_w2v2\n",
    "}\n",
    "y = y_multiclass  \n",
    "\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "scoring = {\n",
    "    \"accuracy\": \"accuracy\",\n",
    "    \"precision\": make_scorer(precision_score, average=\"weighted\", zero_division=0),\n",
    "    \"recall\": make_scorer(recall_score, average=\"weighted\", zero_division=0),\n",
    "    \"f1\": make_scorer(f1_score, average=\"weighted\", zero_division=0)\n",
    "}\n",
    "\n",
    "model_dir = \"../model/grid_search_multiclass\"\n",
    "eval_results = []\n",
    "\n",
    "for fname in os.listdir(model_dir):\n",
    "    if fname.endswith(\".pkl\") and \"_\" in fname:\n",
    "        model_name, vectorizer_name = fname.replace(\".pkl\", \"\").split(\"_\", 1)\n",
    "        model_path = os.path.join(model_dir, fname)\n",
    "        model = joblib.load(model_path)\n",
    "\n",
    "        if vectorizer_name not in datasets:\n",
    "            print(f\"Dataset '{vectorizer_name}' not found.\")\n",
    "            continue\n",
    "\n",
    "        X = datasets[vectorizer_name]\n",
    "\n",
    "        try:\n",
    "            scores = cross_validate(\n",
    "                model,\n",
    "                X,\n",
    "                y,\n",
    "                cv=cv,\n",
    "                scoring=scoring,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating {model_name} with {vectorizer_name}: {e}\")\n",
    "            scores = {}\n",
    "\n",
    "        # Calcolo delle metriche con nan-safe mean\n",
    "        result = {\n",
    "            \"model\": model_name,\n",
    "            \"vectorizer\": vectorizer_name\n",
    "        }\n",
    "\n",
    "        for key in [\"accuracy\", \"precision\", \"recall\", \"f1\"]:\n",
    "            score_values = scores.get(f\"test_{key}\", [np.nan])\n",
    "            mean_score = np.nanmean(score_values)\n",
    "            result[key] = mean_score\n",
    "\n",
    "            if np.isnan(mean_score):\n",
    "                print(f\" {model_name} + {vectorizer_name}: '{key}' is NaN\")\n",
    "\n",
    "        eval_results.append(result)\n",
    "\n",
    "# Visualizzazione risultati\n",
    "df_eval = pd.DataFrame(eval_results)\n",
    "df_eval_sorted = df_eval.sort_values(by=\"f1\", ascending=False)\n",
    "\n",
    "display(df_eval_sorted)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DMML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
